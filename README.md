# Public_Health
  Project status(Active)

# Project objective
  This project embarks on an analytical journey into a public health dataset sourced from Kaggle, specifically from the "Heart Disease Dataset" available at https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset. The primary objective is to leverage various data science techniques to understand, explore, and ultimately predict the presence of heart disease based on a comprehensive set of patient attributes. Our methodology involves a multi-faceted approach, beginning with rigorous data cleaning to ensure data quality and consistency. Following this, we delve into exploratory data analysis (EDA) to uncover patterns, distributions, and initial relationships within the data through various visualizations. The core of the project then shifts to predictive modeling, where we aim to build robust classification models to accurately identify individuals at risk of heart disease. Furthermore, we will conduct feature importance analysis to pinpoint the most influential factors contributing to the predictions, perform hypothesis testing to statistically validate observed relationships, and explore potential patient segments through unsupervised clustering, all to derive actionable insights from this critical health data.

# Methods
  List with methods:
  - Data Cleaning and Preprocessing:
    - Handling Missing Values (Imputation)
    - Removing Duplicate Rows
    - Correcting Data Types
  - Descriptive Statistics:
    - Calculating Summary Statistics (mean, median, std, min, max, quartiles) for numerical features.
    - Generating Frequency Counts and Percentages for categorical features.
  - Exploratory Data Analysis (EDA):
    - Visualizing Distributions (Histograms, KDE plots for numerical data; Count plots for categorical data).
    - Analyzing Relationships between Features and Target (Box plots, Violin plots, Count plots with hue, Stacked Bar Charts).
    - Creating Correlation Heatmaps.
    - Generating Pair Plots (for numerical features).
  - Predictive Modeling (Classification):
    - Data Splitting (Training and Testing sets).
    - Feature Scaling.
    - Training Classification Models (e.g., Logistic Regression, Random Forest Classifier).
    - Model Evaluation (Accuracy, Precision, Recall, F1-Score, ROC AUC, Confusion Matrix, Classification Report).
  - Feature Importance Analysis:
    - Extracting Model-based Feature Importances (from Random Forest).
    - Calculating Permutation Importance.
    - Visualizing Feature Importances.
  - Segmentation/Clustering (Unsupervised Learning):
    - Determining Optimal Number of Clusters (Elbow Method).
    - Applying K-Means Clustering.
    - Analyzing Cluster Characteristics (mean feature values per cluster, categorical distributions).
    - Visualizing Clusters (using PCA for dimensionality reduction).
  - Hypothesis Testing:
    - Independent Samples T-tests (comparing numerical means between two groups).
    - Chi-squared Tests of Independence (assessing association between two categorical variables).
  - Comprehensive Data Visualization:
    - Creating various plots to illustrate data distributions and relationships, often with the target variable as a focus.

# Technologies 
  List with used technologies, ex:
  - Python
  - Pandas
  - MySQL

# Project Description
  Paragraph with a description of the dataset, sources, characteristics ,etc.

# Steps
  Add here any insights you had during the project

# Conclusion
  Final conclusion
  
# Contact
  linkedin, github, medium, etc 