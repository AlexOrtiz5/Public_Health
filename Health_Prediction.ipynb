{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f433a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8deca",
   "metadata": {},
   "source": [
    "# --- 1. Load the Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a564f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "      <th>cp_0</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0     52    1       125   212    0        1      168      0      1.0      2   \n",
       "1     53    1       140   203    1        0      155      1      3.1      0   \n",
       "2     70    1       145   174    0        1      125      1      2.6      0   \n",
       "3     61    1       148   203    0        1      161      0      0.0      2   \n",
       "4     62    0       138   294    1        1      106      0      1.9      1   \n",
       "..   ...  ...       ...   ...  ...      ...      ...    ...      ...    ...   \n",
       "297   68    0       120   211    0        0      115      0      1.5      1   \n",
       "298   44    0       108   141    0        1      175      0      0.6      1   \n",
       "299   52    1       128   255    0        1      161      1      0.0      2   \n",
       "300   59    1       160   273    0        0      125      0      0.0      2   \n",
       "301   54    1       120   188    0        1      113      0      1.4      1   \n",
       "\n",
       "     ca  thal  target  cp_0  cp_1  cp_2  cp_3  \n",
       "0     2     3       0     1     0     0     0  \n",
       "1     0     3       0     1     0     0     0  \n",
       "2     0     3       0     1     0     0     0  \n",
       "3     1     3       0     1     0     0     0  \n",
       "4     3     2       0     1     0     0     0  \n",
       "..   ..   ...     ...   ...   ...   ...   ...  \n",
       "297   0     2       1     0     0     1     0  \n",
       "298   0     2       1     0     0     1     0  \n",
       "299   1     3       0     1     0     0     0  \n",
       "300   0     2       0     0     0     0     1  \n",
       "301   1     3       0     1     0     0     0  \n",
       "\n",
       "[302 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data that is inside of the CSV\n",
    "df = pd.read_csv(\"./Health_Data/cleaned_health.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3680f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y) columns\n",
    "# These are the columns from your dataset that will be used for training\n",
    "FEATURES = [\n",
    "    'age', 'sex', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "    'exang', 'oldpeak', 'slope', 'ca', 'thal',\n",
    "    'cp_0', 'cp_1', 'cp_2', 'cp_3'\n",
    "]\n",
    "TARGET_COL = 'target' # The column indicating disease presence (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba7fdc",
   "metadata": {},
   "source": [
    "# --- 2. Data Preprocessing (Ensuring Cleanliness and Correct Types) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc100584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Preprocessing ---\n",
      "Missing values before imputation for modeling:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values after imputation for modeling:\n",
      "age         0\n",
      "sex         0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "cp_0        0\n",
      "cp_1        0\n",
      "cp_2        0\n",
      "cp_3        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Features (X) shape: (302, 16)\n",
      "Target (y) shape: (302,)\n",
      "Target distribution:\n",
      "1    0.54\n",
      "0    0.46\n",
      "Name: target, dtype: Float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Data Preprocessing ---\")\n",
    "\n",
    "# Handle '?' or other non-numeric values if they exist, converting to NaN first\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "\n",
    "# Convert all feature and target columns to numeric, coercing errors\n",
    "all_relevant_cols = FEATURES + [TARGET_COL]\n",
    "for col in all_relevant_cols:\n",
    "    if col in df.columns:\n",
    "        # For numerical columns, use float type initially to handle NaNs, then convert to Int64 if appropriate\n",
    "        if col in ['sex', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'cp_0', 'cp_1', 'cp_2', 'cp_3', TARGET_COL]:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64') # Use nullable integer\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Impute missing values after type conversion\n",
    "print(\"Missing values before imputation for modeling:\")\n",
    "print(df[all_relevant_cols].isnull().sum()[df[all_relevant_cols].isnull().sum() > 0])\n",
    "\n",
    "for col in all_relevant_cols:\n",
    "    if col in df.columns and df[col].isnull().any():\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Use median for numerical features, mode for categorical/target\n",
    "            if col in NUMERICAL_COLS: # Assuming NUMERICAL_COLS is defined from EDA script\n",
    "                median_val = df[col].median()\n",
    "                df[col].fillna(median_val, inplace=True)\n",
    "                print(f\"Filled missing values in '{col}' with its median ({median_val}).\")\n",
    "            else: # Categorical/binary features including target\n",
    "                mode_val = df[col].mode()[0]\n",
    "                df[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"Filled missing values in '{col}' with its mode ({mode_val}).\")\n",
    "        else:\n",
    "            # Fallback for non-numeric (shouldn't happen if type conversion is robust)\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"Filled missing non-numeric values in '{col}' with its mode ({mode_val}).\")\n",
    "\n",
    "print(\"\\nMissing values after imputation for modeling:\")\n",
    "print(df[all_relevant_cols].isnull().sum())\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Check if target variable has only two unique values (binary classification)\n",
    "if y.nunique() != 2:\n",
    "    print(f\"Error: The target column '{TARGET_COL}' is not binary. It has {y.nunique()} unique values: {y.unique()}\")\n",
    "    print(\"Please ensure your 'target' column is binary (e.g., 0 and 1) for classification.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71298c42",
   "metadata": {},
   "source": [
    "# --- 3. Split Data into Training and Testing Sets ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4550f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (226, 16), (226,)\n",
      "Testing set shape: (76, 16), (76,)\n"
     ]
    }
   ],
   "source": [
    "# We use a stratified split to ensure that the proportion of target classes\n",
    "# is roughly the same in both training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"\\nTraining set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df8a55",
   "metadata": {},
   "source": [
    "# --- 4. Feature Scaling (for numerical features) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1b2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Scaling ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NUMERICAL_COLS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Identify numerical features that are actually present in the DataFrame\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m numerical_features_in_X \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mNUMERICAL_COLS\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numerical_features_in_X:\n\u001b[0;32m     10\u001b[0m     X_train[numerical_features_in_X] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train[numerical_features_in_X])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NUMERICAL_COLS' is not defined"
     ]
    }
   ],
   "source": [
    "# Scaling is crucial for algorithms sensitive to feature magnitudes (e.g., Logistic Regression, SVM, Neural Networks)\n",
    "# Tree-based models (Random Forest) are generally not sensitive to scaling.\n",
    "print(\"\\n--- Feature Scaling ---\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify numerical features that are actually present in the DataFrame\n",
    "numerical_features_in_X = [col for col in NUMERICAL_COLS if col in X_train.columns]\n",
    "\n",
    "if numerical_features_in_X:\n",
    "    X_train[numerical_features_in_X] = scaler.fit_transform(X_train[numerical_features_in_X])\n",
    "    X_test[numerical_features_in_X] = scaler.transform(X_test[numerical_features_in_X])\n",
    "    print(f\"Scaled numerical features: {numerical_features_in_X}\")\n",
    "else:\n",
    "    print(\"No numerical features found for scaling.\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of scaled training features:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f53aab",
   "metadata": {},
   "source": [
    "# --- 5. Model Training and Evaluation ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e8a0b",
   "metadata": {},
   "source": [
    "# --- 5.1. Logistic Regression ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training and Evaluating Logistic Regression Model ---\")\n",
    "log_reg_model = LogisticRegression(random_state=42, solver='liblinear') # liblinear is good for small datasets\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg_model.predict(X_test)\n",
    "y_proba_log_reg = log_reg_model.predict_proba(X_test)[:, 1] # Probability of target=1\n",
    "\n",
    "print(\"\\nLogistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_log_reg):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba_log_reg):.4f}\")\n",
    "print(\"\\nConfusion Matrix (Logistic Regression):\")\n",
    "print(confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(\"\\nClassification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2021c",
   "metadata": {},
   "source": [
    "# --- 5.2. Random Forest Classifier ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training and Evaluating Random Forest Classifier Model ---\")\n",
    "# RandomForestClassifier is a robust ensemble method\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced') # class_weight helps with imbalance\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_proba_rf = rf_model.predict_proba(X_test)[:, 1] # Probability of target=1\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
    "print(\"\\nConfusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3a823",
   "metadata": {},
   "source": [
    "# --- 6. Feature Importance (for Random Forest) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Feature Importance from Random Forest Model ---\")\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    feature_importances = pd.Series(rf_model.feature_importances_, index=FEATURES).sort_values(ascending=False)\n",
    "    print(feature_importances)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=feature_importances, y=feature_importances.index, palette='viridis')\n",
    "    plt.title('Random Forest Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importances not available for this model type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPredictive modeling and evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
